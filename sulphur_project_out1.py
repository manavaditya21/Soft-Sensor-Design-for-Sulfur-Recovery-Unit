# -*- coding: utf-8 -*-
"""Sulphur_project_Out1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ubNrqnDcUF7u666CayqLr36yjccUQVyv

# OUTPUT 1


1.   Multiple Linear Regression
2.   Polynomial Regression
3.   Lasso Regression
4.   PCR (Principal Component Analysis)
5.   LSTM (Long Short Term Memory)
6.   KNN

# Data Preprocessing
"""

import pandas as pd
from scipy import stats
from scipy.special import comb
import numpy as np
import matplotlib.pyplot as plt
import statistics as st
import math

"""###Import Data"""

path="/content/drive/MyDrive/Machine Learning/CH-512/Project/Table - IN_Table.csv.csv"
data= pd.read_csv(path)
data.describe()

"""###Removing the outliers

"""

#the median method gave higher accuracy than the mean.
def outlier(data):
    for column in data.columns:
        data[column] = data[column].astype(float)
        median = st.median(data[column])
        std_dev = np.std(data[column])
        lower_bound = median - 3*std_dev
        upper_bound = median + 3*std_dev
        data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
    return data
data_c = outlier(data)
data_c.dropna(inplace=True)
data_c.describe()

data_c.dropna(inplace=True)
data_c.describe()

data.corr()

"""###Plotting The Graphs"""

plt.figure(figsize=(80, 20))

for i, col in enumerate(data.columns, start=1):
    plt.subplot(7, 1, i)
    plt.scatter(data_c.index, data_c[col], marker='.', color='C{}'.format(i-1),s=2)
    plt.title(col)

plt.tight_layout()

plt.show()

data_c.corr()

"""### Error Details"""

def results(y1_test,y1_pred,k):
  from sklearn.metrics import mean_squared_error,r2_score

  SSE= mean_squared_error(y1_test,y1_pred)
  MSE= np.mean((y1_test - y1_pred) **2)
  R2 = r2_score(y1_test,y1_pred)
  n=len(y1_test)

  AIC = 2*k          +n*(math.log(math.pi*2)+math.log(SSE/n)+1)
  BIC = k*math.log(n)+n*(math.log(math.pi*2)+math.log(SSE/n)+1)

#Errors
  print('R2  : ', round(R2,4))
  print('SSE : ', round(SSE,4))
  print('MSE : ', round(MSE,4))
  print('aic : ', round(AIC* 10**3)/10**3)
  print('bic : ', round(BIC* 10**3)/10**3)

#To Plot
  plt.figure(figsize=(200,4))
  plt.plot(range(len(y1_test)), y1_test, color='blue', label='Actual Y1')
  plt.plot(range(len(y1_test)), y1_pred, color='red', label='Predicted Y1')
  plt.xlabel('Sample Index')
  plt.ylabel('Y1 Value')
  plt.title('Y1 Actual vs Predicted')
  plt.legend()
  plt.show()

"""#Multivariate Linear Regression

###Loading Data
"""

x  = data_c.loc[:,['IN1','IN2','IN5']]
y1 = data_c.loc[:,'Out1']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x = scaler.fit_transform(x)

from sklearn.model_selection import train_test_split
x_train, x_test, y1_train, y1_test = train_test_split(x, y1,test_size=0.2, random_state=42)

"""### Applying Regression"""

from sklearn.linear_model import LinearRegression
regressor_y1 = LinearRegression()
regressor_y1.fit(x_train, y1_train)

y1_test_values = y1_test.values
y1_pred = regressor_y1.predict(x_test)
print(np.concatenate((y1_pred.reshape(len(y1_pred),1), y1_test_values.reshape(len(y1_test_values),1)),1))

"""### Plotting Result"""

results(y1_test,y1_pred,x.shape[1])

"""# Polynomial Regression

###Loading Data
"""

x  = data_c.loc[:,['IN1','IN2','IN3','IN4','IN5']]
y1 = data_c.loc[:,'Out1']

from sklearn.model_selection import train_test_split
x_train, x_test, y1_train, y1_test = train_test_split(x, y1,test_size=0.2, random_state=42)

"""from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

Degree=7
variable= len(x.columns)
Parameters = comb(variable+Degree, Degree)

poly_reg = PolynomialFeatures(degree = Degree)
x_poly = poly_reg.fit_transform(x_train)

regressor = LinearRegression()
regressor.fit(x_poly, y1_train)

###Applying Regression
"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

Degree=5
variable= len(x.columns)
Parameters = comb(variable+Degree, Degree)

poly_reg = PolynomialFeatures(degree = Degree)
x_poly = poly_reg.fit_transform(x_train)

regressor = LinearRegression()
regressor.fit(x_poly, y1_train)

from sklearn.preprocessing import PolynomialFeatures
y1_test_values = y1_test.values
y1_pred = regressor.predict(poly_reg.transform(x_test))
np.set_printoptions(precision=2)
print(np.concatenate((y1_pred.reshape(len(y1_pred),1), y1_test_values.reshape(len(y1_test),1)),1))

"""### Plotting Results"""

results(y1_test,y1_pred,Parameters)

"""# Lasso regression

###Loading data
"""

X  = data_c.loc[:,['IN1','IN2','IN3','IN4','IN5']]
y1 = data_c.loc[:,'Out1']

from sklearn.model_selection import train_test_split
X_train, X_test, y1_train, y1_test = train_test_split(X,y1, test_size = 0.3, random_state = 0)

"""###Applying Lasso"""

from sklearn.linear_model import LassoCV

# Define a range of alpha values to test
alphas = np.logspace(-4, 4, 100)

# Initialize the LassoCV model
lasso_cv = LassoCV(alphas=alphas, cv=5)  # cv is the number of folds for cross-validation

# Fit the model
lasso_cv.fit(X_train, y1_train)

# Optimal alpha
optimal_alpha = lasso_cv.alpha_
print("Optimal alpha:",optimal_alpha)

from sklearn.linear_model import Lasso
lasso_model = Lasso(alpha= optimal_alpha)
lasso_model.fit(X_train, y1_train)

y1_test_values = y1_test.values
y1_pred = lasso_model.predict(X_test)
print(np.concatenate((y1_pred.reshape(len(y1_pred),1), y1_test_values.reshape(len(y1_test),1)),1))

"""###Plotting Results"""

results(y1_test,y1_pred,3)

"""#PCR"""

from sklearn.decomposition import PCA

X  = data_c.loc[:,['IN1','IN2','IN3','IN4','IN5']]
y = data_c.loc[:,'Out1']

X_train, X_test, y1_train, y1_test = train_test_split(X, y, test_size=0.3, random_state=42)

pca = PCA(n_components=3)
x_train = pca.fit_transform(X_train)
x_test = pca.transform(X_test)

explained_variance = pca.explained_variance_ratio_

print("Explained Variance Ratio:")
sum=0
for i, ratio in enumerate(explained_variance):
    print(f"  Component {i+1}: {ratio:.4f}")
    sum+= ratio
print("Data Contained = ",sum*100,"%")

"""###Applying Regression"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly_reg = PolynomialFeatures(degree = 7)
x_poly = poly_reg.fit_transform(x_train)

regressor = LinearRegression()
regressor.fit(x_poly, y1_train)

y1_test_values = y1_test.values
y1_pred = regressor.predict(poly_reg.transform(x_test))
np.set_printoptions(precision=2)
print(np.concatenate((y1_pred.reshape(len(y1_pred),1), y1_test_values.reshape(len(y1_test),1)),1))

"""### Plotting Results"""

results(y1_test,y1_pred,7)

"""#LSTM Trials

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.callbacks import Callback

"""### Callback Function"""

class R2Callback(Callback):
    def __init__(self, X_val, Y_val):
        super(R2Callback, self).__init__()
        self.X_val = X_val
        self.Y_val = Y_val
        self.r2_scores = []

    def on_epoch_end(self, epoch, logs={}):
        if (epoch + 1) % 200 == 0:  # Calculate R2 score every 50 epochs
            Y_pred = self.model.predict(self.X_val)
            r2 = r2_score(self.Y_val, Y_pred)
            self.r2_scores.append(r2)
            print(f'R2 score at epoch {epoch + 1}: {r2}')

"""###Data Loading & Scaling

"""

scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data_c[['IN1', 'IN2','IN3','IN5', 'Out1']])

X = scaled_data[:, :-1]
Y = scaled_data[:, -1]

X_train, X_test, Y1_train, Y1_test = train_test_split(X, Y, test_size=0.2, random_state=42)

X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

"""### Applying Regression"""

model = Sequential()
model.add(LSTM(200, activation='relu', input_shape=(1, 4)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mae')

r2_callback = R2Callback(X_test, Y1_test)

model.fit(X_train, Y1_train, epochs=1000, batch_size=64, verbose=5, callbacks=[r2_callback])

loss = model.evaluate(X_test, Y1_test, verbose=2)
print(f'Test loss: {loss}')

Y1_pred = model.predict(X_test)

"""### Plotting results"""

results(Y1_test,Y1_pred,4)

"""# KNN

### Applying KNN
"""

import pandas as pd
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


X = data_c.loc[:,['IN1','IN2','IN3','IN4','IN5']]
y = data_c.loc[:,'Out1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


knn = KNeighborsRegressor(n_neighbors=5)

"""### LInear"""

knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

results(y_test,y_pred,3)

"""### PCA Based Linear"""

pca = PCA(n_components=3)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)


knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_pca, y_train)
y_pred = knn.predict(X_test_pca)

results(y_test,y_pred,3)

"""### Polynomial"""

import pandas as pd
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.pipeline import Pipeline


X = data_c.loc[:, ['IN1', 'IN2', 'IN5', 'IN3', 'IN4']]
y = data_c.loc[:, 'Out1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a pipeline with polynomial features, scaling, and KNN regression
polynomial_degree = 3  # You can adjust the degree of the polynomial

polynomial_regression = Pipeline([
    ('poly', PolynomialFeatures(degree=polynomial_degree)),
    ('scaler', StandardScaler()),
    ('knn', KNeighborsRegressor(n_neighbors=5))
])

# Train the pipeline
polynomial_regression.fit(X_train, y_train)

# Make predictions on the test set
y_pred = polynomial_regression.predict(X_test)


results(y_test,y_pred,3)